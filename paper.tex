% Template for ICIP-2013 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,amsmath,graphicx}

% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}
\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}

% Title.
% ------
\title{Investigation on Parallel Training of Deep Neural Network using Model Averaging}
%
% Single address.
% ---------------
\name{Hang Su$^{1,2}$, Haoyu Chen$^1$, Haihua Xu$^3$}
\address{$^1$ International Computer Science Institute, Berkeley, California, US \\
$^2$ Dept. of Electrical Engineering \& Computer Science, University of California, Berkeley, CA, USA \\
$^3$ Nanyang Technological University, Singapore \\
{\small \tt \{suhang3240@gmail.com\}}
}
%
% For example:
% ------------
%\address{School\\
%	Department\\
%	Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
%\twoauthors
%  {A. Author-one, B. Author-two\sthanks{Thanks to XYZ agency for funding.}}
%	{School A-B\\
%	Department A-B\\
%	Address A-B}
%  {C. Author-three, D. Author-four\sthanks{The fourth author performed the work
%	while at ...}}
%	{School C-D\\
%	Department C-D\\
%	Address C-D}
%
\begin{document}
%\ninept
%
\maketitle
%
\begin{abstract}
In this work we apply model averaging to parallel training of deep neural network (DNN). 
Parallelization is done in a model averaging manner. Data is partitioned and distributed to different nodes for 
local model updates, and model averaging across nodes is done every few minibatches. 

We use multiple GPUs for data parallelization, and Message Passing Interface (MPI) for communication between nodes,
which allows us to perform model averaging frequently without losing much time on communication.
We investigate the effectiveness of Natrual Gradient for Stochasitc Gradient Descent (NG-SGD) in the model averaging
framework, and compare two learning rate schedule strategies for model update. It is shown that NG-SGD leads to better
convergence in model averaging framework. On the 300h Swithboard dataset, a 9.3 times speedup is achieved using 16 GPUs
and 17 times speedup using 32 GPUs with limited decoding accuracy loss. 
\end{abstract}
%
\begin{keywords}
Parallel training, model averaging, deep neural network
\end{keywords}
%
\section{Introduction}
\label{sec:intro}
Deep Neural Networks (DNN) has shown its effeciveness in several machine learning tasks, espencially in speech
recognition. The large model size and massive training examples make DNN a powerful model for classification. However,
these two factors also slow down the training procedure of DNNs.

Parallelization of DNN training has been a popular topic since the revival of neural networks. Several different strategies
have been proposed to tackle this problem. Multiple thread CPU parallelization and single GPU implementation are compared
in \cite{scanzio2010parallel,vesely2010parallel}, and it is shown that single GPU could beat multi-threaded CPU implementation
by a factor of 2.

Optimality for parallelization of DNN training was analyzed in \cite{seide2014parallelizability}, and based on the analysis, 
a gradient quantization approach (1-bit SGD) was proposed to minimize communication cost \cite{seide20141}. It shows that 1 bit
can effectively reduce the amount of data exchanged in an MPI framework, and a 10 times speed-up using 40 GPUs.

DistBelief proposed in \cite{dean2012large} reports that 8 CPU machines train 2.2 times faster than a single GPU machine on a
moderately sized speech model. Asynchronous SGD using multiple GPUs achieved a 3.2x speed-up on 4 GPUs \cite{zhang2013asynchronous}.

A pipeline training approach was propoased in \cite{chen2012pipelined} and a 3.3x speedup was achieved using 4 GPUs, but this
method does not scale beyond number of layers in the neural network.

A speedup of 6x to 14x was achieved using 16 GPUs on training convolutional neural networks \cite{coates2013deep}. In this approach,
each GPU is responsible for a partition of the neural network. This approach is more useful for image classification where 
local structure of the neural network could be exploited. For a fully connected speech model, a model partition approach 
may not be able to contribute as much.

Distributed model averaging is used in \cite{zhang2014improving,miao2014distributed}, and a further improvement 
is done using NG-SGD \cite{povey2014parallel}. In this approach, separate models are trained on multiple nodes using 
different partitions of data, and model parameters are averaged after each epoch. It is shown that NG-SGD can effectively improve
convergence and ensure a better model trained using the model averaging framework.

Our approach is mainly based on the NG-SGD with model averaging. We utilize multiple GPUs in neural networks training via 
MPI, which allows us to perform model averaging more frequently and efficiently. Unlike the other approach \cite{seide20141},
we do not use a warm-up phase where only single thread is used for model update. (Admittedly, this might lead to further improvement). 
In this work, we conduct a lot of experiments and compare different setups in model averaging framework.

In Section 2, we introduce related works on NG-SGD. Section 3 describe the model averaging approach and some 
intuition on the analysis. Section 4 records experimental results on different setups and Section 5 concludes.

\section{Data parallelization and Model Averging}
SGD is a popular method for DNN training. Even though neural network training objectives are usually non-convex, 
mini-batch SGD has been shown to be effective for optimizing the objective\cite{seide2011conversational}. 
Roughly speaking, a bigger minibatch size gives a better estimate of the gradient, resulting in a better the converge rate. 
Thus, a straight forward idea for parallellization would be distributing the gradient computation to different computing
nodes. In each step, gradients of minibatches on different nodes are reduced to a single node, averaged and then used to 
update models in each node. This method can compute the gradient accurately, but it requires heavy communication between nodes.
Also, it is shown that increasing minibatch size does not always benefit model training\cite{seide2011conversational}, 
especially in early stage of model training.

On the other hand, if we choose to average the parameters rather than gradients, it is not necessary to exchange data that often. 
Currently, there is no straight forward theory that guarantees convergence, but we would like to explore a bit why this strategy 
should work, just as we observe in the experiments.

First, in the extreme case where model parameters are averaged after each weight update, model averaging is equivalent to 
gradient averaging. Furthermore, if model averaging is done every $n$ minibatch based weight update, model update formula could
be written as
\begin{equation}
\begin{split}
\theta_{t+n} &= \theta_{t} +\sum_{i=0}^{n-1} \alpha g_{t+i}\\
&= \theta_{t} +\sum_{i=0}^{n-1} \alpha \frac{\partial}{\partial\theta} F(x;\theta_{t+i})
\end{split}
\end{equation}
\begin{equation}
\overbar{\theta_{t+n}} = \overbar{\theta_{t}} +\sum_{i=0}^{n-1} \alpha \frac{\partial}{\partial\theta} \overbar{F}(x;\theta_{t+i})
\end{equation}
where $\theta$ is the model parameter and $\alpha$ is learning rate. If changes in model parameter $\theta$ is limited 
within $n$ updates, this approach could be seen as an approximation to gradient averaging.

Second, it is shown that model averging for convex models is guaranteed to converge \cite{mcdonald2010distributed,mcdonald2009efficient}.
It is suggested that unsupervised pretraining guides the learning towards basins of attraction of minima that support better generalization
from the training data set; \cite{erhan2010does}.

Fig~\ref{fig:allreduce} is an example of all-reduce with 4 nodes. This operation could be easily implemented by MPI\_Allreduce.
\begin{figure}[htb]
  \centering
  \includegraphics[width=0.5\textwidth]{allreduce.png}
  \caption{All-reduce network}
  \label{fig:allreduce}
\end{figure}

\section{Natural Gradient for Model Update}
This section introduces the idea proposed in \cite{povey2014parallel}.

In stochastic gradient descent (SGD), the learning rate is often assumed to be a scalar $\alpha_t$ that may change over time,
the update formula for model parameters $\theta_{t}$ is
\begin{equation}
\theta_{t+1} = \theta_{t} + \alpha_t g_t
\end{equation}
where $g_t$ is the gradient.

However, according to Natural Gradient idea \cite{murata1999statistical,roux2008topmoumoute}, it is possible to replace 
the scalar with a symmetric positive definite matrix $E_t$, which is the inverse of the Fisher information matrix.
\begin{equation}
\theta_{t+1} = \theta_{t} + \alpha_t E_t g_t
\end{equation}

Suppose $x$ is the variable we are modeling, and $f(x;\theta)$ is the probability or likelihood of $x$ given parameters $\theta$, then the
Fisher information matrix $I(\theta)$ is defined as
\begin{equation}
\frac{\partial}{\partial\theta}\log f(x;\theta)
\end{equation}

For large scale speech recognition, it is impossible to estimate Fisher information matrix and perform inversion, 
so it is necessary to approximate the inverse Fisher information matrix directly. Details about the theory and 
implementation of NG-SGD could be found in \cite{povey2014parallel}.

\section{Experimental Results}
\subsection{Setup}
In this work, we report speech recognition results on the 300 hour Switchboard conversational telephone speech task 
(Switchboard-1 Release 2). We use MSU-ISIP release of the Switchboard segmentations and transcriptions (date 11/26/02),
together with the Mississippi State transcripts2 and the 30Kword lexicon released with those transcripts. 
The lexicon contains pronunciations for all words and word fragments in the training data. We use the Hub5 ’00 data for
evaluation. Specifically, we use the  the development set and Hub5 ’01 (LDC2002S13) data as a separate test set.

The Kaldi toolkit\cite{kaldi11} is used for speech recognition framework. Standard 13-dim PLP feature,
together with 3-dim Kaldi pitch feature, is extracted and used for maximum
likelihood GMM model training. Features are then transformed using LDA+MLLT before SAT training.
After GMM training is done, a tanh-neuron DNN-HMM hybrid system is trained using the the 40-dimension 
transformed fMLLR (also known as CMLLR \cite{gales1996generation}) feature as input and GMM-aligned senones 
as targets. fMLLR is estimated in an EM fashion for both training data and test data. A trigram language model (LM) is trained 
on 3M words of the training transcripts only.

Work in this paper is built on top of the Kaldi nnet1 setup and the NG-SGD method introduced in nnet2 setup. 
Details of DNN training follows Section 2.2 in \cite{vesely2013sequence}. In this work, we use 6 hidden layers, where each 
hidden layer has 2048 neurons with sigmoids. Input layer is 440 dimension (i.e. the context of 11 fMLLR frames), 
and output layer is 8806 dimension. Mini-batch SGD is used for backpropagation and the minibatch is set to 1024 for all
the experiments. By defult, DNNs are initialized with stacked restricted Boltzmann machines (RBMs) that are pretrained 
in a greedy layerwise fashion \cite{hinton2006fast}. Comparison between random initialization and RBM-initialization 
in model averaging framework is reported in Section~\ref{sec:init}.

The server hardware used in this work is Stampede (TACC) (URL: https://portal.xsede.org/tacc-stampede). It is a Dell Linux 
cluster provided as an Extreme Science Engineering Discovery Environment (XSEDE) digital service by the Texas Advanced 
Computing Center (TACC). Stampede is configured with 6,400 Dell DCS Zeus compute nodes, the majority of which are configured
with two 2.7 GHz E5-2680 Intel Xeon (Sandy Bridge) processors and one Intel Xeon Phi SE10P coprocessor. 128 of the nodes are 
augmented with an NVIDIA K20 GPU and 8 GB of GDDR5 memory each, which we use for neural network training in this work.
Stampede nodes run Linux 2.6.32 with batch services managed by the Simple Linux Utility for Resource Management (SLURM).

\subsection{Switchboard Results}
scaling factor
\begin{figure}[htb]
  \centering
  \includegraphics[width=0.5\textwidth]{scaling.png}
  \caption{Scaling factor and speedup factor v.s. number of gpus}
  \label{fig:scaling}
\end{figure}

percentage of time spent on communication
\begin{figure}[htb]
  \centering
  \includegraphics[width=0.5\textwidth]{percent.png}
  \caption{Percentage of time on MPI communication v.s. number of gpus}
  \label{fig:percent}
\end{figure}

\begin{table}
  \centering
  \begin{tabular}{c|c|c|c|c|c|c}
    \hline
     Nodes  & 1    & 2    & 4    & 8    & 16 & 32\\
    \hline
Allreduce &    14.7 & -- & -- & 15.1 & 15.1   & 15.2\\
    \hline
Butterfly &    14.7 & -- & -- & --   & 15.6   & 16.0\\
    \hline
  \end{tabular}
  \caption{Comparison of WERs using different number of GPUs }
  \label{tab:wer}
\end{table}

It is shown that this is

\subsection{Initialization Matters}
\label{sec:init}
Compare random init and dbn init.

\begin{table}
  \centering
  \begin{tabular}{c|c|c|c|c}
    \hline
    & \multicolumn{2}{c|}{SWB}  & \multicolumn{2}{c}{CallHome} \\
    \hline
    Nodes          &  1     & 32     &   1    & 32\\
    \hline
    random init     & RR    & 16.4   &  RR    & 28.8 \\
    \hline
    rbm pretraining & 14.7  & 15.2   & 26.8   & 27.1 \\
    \hline
  \end{tabular}
  \caption{Comparing rbm pretraining with random initialization}
  \label{tab:init}
\end{table}

\subsection{Averaging frequency}
Averaging frequency here is defined as the number of minibatch-SGD performed per model averaging.
Due to the limitation of computing resource, we only did preliminary experiments on this. Minibatch size of 1024 
is set as default, and we compare averaging frequency of 10 and 20. It is shown that an averaging frequency of 10 gives
slight worse speedup but a better decoding WER. The tradeoff between lower averaging frequency (i.e. better speedup) and
better training accuracy is within expectation in that frequent model averaging means steady gradient estimation.

\begin{table}
  \centering
  \begin{tabular}{c|c|c|c}
    \hline
    frequency   & Speedup   & SWB   & CallHome \\
    \hline
    baseline    &   --      & 14.7  & 26.8\\
    \hline
    10          & 9.32      & 15.1  & 27.0 \\
    \hline
    20          & 10.07     & 15.8  & 28.0 \\
    \hline
  \end{tabular}
  \caption{Comparing different averaging frequencies}
  \label{tab:init}
\end{table}

\subsection{Minibatch Size}
\begin{table}
  \centering
  \begin{tabular}{c|c|c|c|c|c}
    \hline
    & \multicolumn{2}{c|}{SWB}    & \multicolumn{2}{c}{CallHome} & Speedup\\
    \hline
    nodes       &  1    &   16    &    1       &   16   &    \\
    \hline
    256         &  --      & 15.1  & 27.0     &   & \\
    \hline
    1024        & 9.32     & 15.1  & 27.0     &   & \\
    \hline
  \end{tabular}
  \caption{Comparing different minibatch size}
  \label{tab:init}
\end{table}


\subsection{Learing Rate Schedule}
Initial learning rate is increased in porportion to number of threads in this setup. The reason for this is straight forward:
Assume we have $n$ minibatches of data for model training. When the model is trained using single thread, it gets updated $n$
times. When data is distributed to $m$ machines, then each model gets updated $n/m$ times. Since the effect of model averaging 
is mostly aggregating knowledge learnt from different data partition, the absolute change of model shall be compensated by 
$m$ times.

We compare two learning rate schedules in this section. The first one is the default setup used in Kaldi nnet1 (Newbob). 
It starts with a initial learning rate of 0.32 and halves the rate when the improvement in frame accuracy on a cross-validation 
set between two successive epochs falls below 0.5\%. The optimization terminates when the frame accuracy increases by less 
than 0.1\%. Cross-validation is done on 10\% of the utterances that are held out from the training data.

The second learing rate schedule is exponentially decaying. This method is used in \cite{senior2013empirical,povey2014parallel} 
and is shown to be superior to performance scheduling and power scheduling. In this work, it starts with the same initial learning
rate as the first method (Newbob), and decrease to the final learning rate (which is set to be 0.01 * initial learning rate). The
number of epochs is set to $15$ in this task, which is set to be the same as Newbob scheduling.

\begin{table}
  \centering
  \begin{tabular}{c|c|c|c|c}
    \hline
    & \multicolumn{2}{c|}{SWB}  & \multicolumn{2}{c}{CallHome} \\
    \hline
    Nodes       &  1      & 16    &  1    & 16 \\
    \hline
    Newbob      & 14.9    & 15.4  & 26.6  & 27.2 \\
    \hline
    exponential & 14.7    & 15.1  & 26.8  & 27.0 \\
    \hline
  \end{tabular}
  \caption{Comparing learning rate schedule}
  \label{tab:init}
\end{table}

\subsection{Online NG-SGD Matters}
\begin{table}
  \centering
  \begin{tabular}{c|c|c|c|c}
    \hline
    & \multicolumn{2}{c|}{SWB}  & \multicolumn{2}{c}{CallHome} \\
    \hline
     Nodes  &  1    & 16        &   1   &  16 \\
    \hline
    SGD     & 14.9    & 16.3    &  26.9 & 28.3 \\
    \hline
    NG-SGD  & 14.7    & 15.1    &  26.8 & 27.0  \\
    \hline
  \end{tabular}
  \caption{Comparing NG-SGD and naive SGD}
  \label{tab:ngsgd}
\end{table}

\section{Conclusion and Future Work}
Neural network training can be efficiently speed up by using model averaging techniques and NG-SGD. 
Butterfly-mixing and ring reduction can reduce communication time a lot, but the models may not converge as well as 
Allreduce when the number of jobs goes up to 16.

Further improvement might be achieved if parallel training runs on top of serial training initialization.
CUDA aware MPI, learning rate schedule. Averaging frequency. Theory on convergence.

\section{Acknowledgements}
We would like to thank Karel Vesely and Daniel Povey who wrote the original "nnet1" neural network training code
and natural gradient stochastic gradient descent upon which the work here is based. We would also like to thank Nelson 
Morgan, Forrest Iandola and Yuansi Chen for their helpful suggestions. 

We acknowledge the Texas Advanced Computing Center (TACC) at The University of Texas at Austin for providing 
HPC resources that have contributed to the research results reported within this work (URL: http://www.tacc.utexas.edu).

% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\bibliographystyle{IEEEbib}
\bibliography{paper}

\end{document}
